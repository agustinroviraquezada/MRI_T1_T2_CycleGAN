{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Legacy"
      ],
      "metadata": {
        "id": "MoUQ94FuqlBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def EncLayer(self,Ich,Och,k_size=3,p=1,p_m='reflect',s=2,relu=True,drop=None,norm=True):\n",
        "  '''\n",
        "  @Description\n",
        "    Creates dk layer using a convoutional layer, with ot without normalizaton.\n",
        "    This is the downsampling part, in other words the layer of the encoder\n",
        "\n",
        "  @Inputs\n",
        "    Ich: Input channels, int\n",
        "    Och: Output channles, int\n",
        "    k_size. kernel size By default 3\n",
        "    p. Padding, by default 1\n",
        "    p_m. Padding mode, by default 'reflect'\n",
        "    s. Stride, as is defined in [1] it is 2 as default\n",
        "    relu. Boolean, True as defaul uses Relu. False use nn.LeakyReLU(0.2)\n",
        "    drop.  by default is None. Float number indicates the percentage of a dropout layer\n",
        "    norm. by default is True for instance normalization. False indicates no instance normalization layer.\n",
        "\n",
        "  @Outputs\n",
        "    Sequential model wich correspond a dk layer \n",
        "  '''\n",
        "  m = nn.Sequential()\n",
        "  m.add_module(\"conv1\", nn.Conv2d(Ich, \n",
        "                            Och, \n",
        "                            kernel_size=k_size, \n",
        "                            padding=p, \n",
        "                            stride=s, \n",
        "                            padding_mode=p_m))\n",
        "  \n",
        "  if norm: m.add_module(\"Instancenorm\", nn.InstanceNorm2d(Och)) \n",
        "  m.add_module(\"activation\", nn.ReLU()) if relu else m.add_module(\"activation\", nn.LeakyReLU(0.2))\n",
        "  if drop: m.add_module(\"Dropout\", nn.Dropout(drop)) \n",
        "  return m\n",
        "\n",
        "\n",
        "def DecLayer(self,Ich,Och,k_size=3,p=1,s=2,relu=True,drop=None,norm=True,upsampling=None):\n",
        "  '''\n",
        "  @Description\n",
        "    Creates uk layer using a deconvoutional layer, with or without normalization.\n",
        "    This is the upsampling part, in other words the layer of the decoder\n",
        "\n",
        "  @Inputs\n",
        "    Ich: Input channels, int\n",
        "    Och: Output channles, int\n",
        "    k_size. kernel size By default 3\n",
        "    p. Padding, by default 1\n",
        "    p_m. Padding mode, by default 'reflect'\n",
        "    s. Stride, as is defined in [1] it is 2 as default\n",
        "    relu. Boolean, True as defaul uses Relu. False use nn.LeakyReLU(0.2)\n",
        "    drop.  by default is None. Float number indicates the percentage of a dropout layer\n",
        "    norm. by default is True for instance normalization. False indicates no instance normalization layer.\n",
        "    upsampling. by default is None. int number indicates the scale factor for upsampling\n",
        "\n",
        "  @Outputs\n",
        "    Sequential model wich correspond a uk layer \n",
        "  '''\n",
        "  m = nn.Sequential()\n",
        "  if upsampling: m.add_module(\"upsampling\",nn.Upsample(scale_factor=upsampling)) #scale_factor =2\n",
        "  m.add_module(\"dconv1\",  nn.ConvTranspose2d(Ich,\n",
        "                                              Och, \n",
        "                                              kernel_size=k_size,\n",
        "                                              padding=p,\n",
        "                                              stride=s))\n",
        "  \n",
        "  if norm: m.add_module(\"Instancenorm\", nn.InstanceNorm2d(Och)) \n",
        "  m.add_module(\"activation\", nn.ReLU()) if relu else m.add_module(\"activation\", nn.LeakyReLU(0.2))\n",
        "  if drop: m.add_module(\"Dropout\", nn.Dropout(drop)) \n",
        "  return m\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "class DecLayer(nn.Module):\n",
        "  '''\n",
        "    @Description\n",
        "      Creates uk layer using a deconvoutional layer, with or without normalization.\n",
        "      This is the upsampling part, in other words the layer of the decoder\n",
        "\n",
        "    @Inputs\n",
        "      Ich: Input channels, int\n",
        "      Och: Output channles, int\n",
        "      k_size. kernel size By default 3\n",
        "      p. Padding, by default 1\n",
        "      p_m. Padding mode, by default 'reflect'\n",
        "      s. Stride, as is defined in [1] it is 2 as default\n",
        "      relu. Boolean, True as defaul uses Relu. False use nn.LeakyReLU(0.2)\n",
        "      drop.  by default is None. Float number indicates the percentage of a dropout layer\n",
        "      norm. by default is True for instance normalization. False indicates no instance normalization layer.\n",
        "      upsampling. by default is None. int number indicates the scale factor for upsampling\n",
        "\n",
        "    @Outputs\n",
        "      Sequential model wich correspond a uk layer \n",
        "    '''\n",
        "  def __init__(self,Ich,Och,nor=True,k=3, s=2, p=1,op=1):\n",
        "      super(DecLayer, self).__init__()\n",
        "      self.conv1 = nn.ConvTranspose2d(Ich,Och, kernel_size=k, stride=s, padding=p, output_padding=op)\n",
        "      if nor:\n",
        "          self.instancenorm = nn.InstanceNorm2d(Och)\n",
        "      self.nor = nor\n",
        "      self.activation = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.conv1(x)\n",
        "      if self.nor:\n",
        "          x = self.instancenorm(x)\n",
        "      x = self.activation(x)\n",
        "      return x\n",
        "\n",
        "class EncLayer(nn.Module):\n",
        "  '''\n",
        "  @Description\n",
        "    Creates dk layer using a convoutional layer, with ot without normalizaton.\n",
        "    This is the downsampling part, in other words the layer of the encoder\n",
        "\n",
        "  @Inputs\n",
        "    Ich: Input channels, int\n",
        "    Och: Output channles, int\n",
        "    k. kernel size By default 3\n",
        "    p. Padding, by default 1\n",
        "    s. Stride, as is defined in [1] it is 2 as default\n",
        "    relu. Boolean, True as defaul uses Relu. False use nn.LeakyReLU(0.2)\n",
        "    drop.  by default is None. Float number indicates the percentage of a dropout layer\n",
        "    norm. by default is True for instance normalization. False indicates no instance normalization layer.\n",
        "\n",
        "  @Outputs\n",
        "    Sequential model wich correspond a dk layer \n",
        "  '''\n",
        "  def __init__(self,Ich,Och,nor=True, k=3,p=1,s=2,act='relu'):\n",
        "    super(EncLayer, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(Ich, Och, kernel_size=k, padding=p, stride=s, padding_mode='reflect')\n",
        "    self.activation = nn.ReLU() if act == 'relu' else nn.LeakyReLU(0.2)\n",
        "    if nor:\n",
        "        self.instancenorm = nn.InstanceNorm2d(Och)\n",
        "    self.nor = nor\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    if self.nor:\n",
        "        x = self.instancenorm(x)\n",
        "    x = self.activation(x)\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "A_7pEhOLqkn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_3KDYwN3D2W"
      },
      "source": [
        "## Install and import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bn-aF3ih3D2W",
        "outputId": "8fecf3ef-0b13-44b5-9543-9af0ec0ac5b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.7/129.7 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 kB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.0/719.0 kB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.4/269.4 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#!pip install --q light-the-torch && ltt install torch\n",
        "!pip install lightning -q\n",
        "!pip install split-folders -q\n",
        "!pip install fastkde -q\n",
        "!pip install torchmetrics -q\n",
        "!pip install pytorch-fid -q\n",
        "!pip install tensorboard -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uT7vhWeC3D2X"
      },
      "outputs": [],
      "source": [
        "#Packages to manage data\n",
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "import gzip\n",
        "import glob\n",
        "import os.path\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#from glob import glob\n",
        "import gzip\n",
        "import shutil\n",
        "import splitfolders\n",
        "import re\n",
        "\n",
        "#Packages to manage Image and compute histograms\n",
        "import fnmatch\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from fastkde.fastKDE import pdf\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import nibabel as nib\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
        "\n",
        "\n",
        "#Package for Lightning\n",
        "import lightning.pytorch as pl\n",
        "from lightning.pytorch.callbacks import EarlyStopping,ModelCheckpoint\n",
        "from lightning.pytorch.callbacks import Callback\n",
        "from lightning.pytorch.loggers import TensorBoardLogger\n",
        "\n",
        "\n",
        "#Package for torch\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision.models as models\n",
        "from pytorch_fid import fid_score\n",
        "import torchvision.utils as vutils\n",
        "from torchmetrics import StructuralSimilarityIndexMeasure as SSIM\n",
        "from torchmetrics import PeakSignalNoiseRatio as PSNR\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#Other packages\n",
        "from skimage import color\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from time import time\n",
        "import multiprocessing as mp\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Important Variables and constants\n",
        "seed=478"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5eB8s403LQ3",
        "outputId": "ac605148-d32a-4f0d-e3de-ed4387d1bb6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGvkwaBQudfO"
      },
      "source": [
        "## Pre-Cleanning  - local \n",
        "\n",
        "\n",
        "Dataset1: https://openneuro.org/datasets/ds002330/versions/1.1.0   \n",
        "Dataset2: https://openneuro.org/datasets/ds002382/versions/1.0.1   \n",
        "Dataset3: https://www.kaggle.com/datasets/awsaf49/brats20-dataset-training-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKhnE90QucNO"
      },
      "outputs": [],
      "source": [
        "local = False\n",
        "\n",
        "if local:\n",
        "  ######################### Move Directory\n",
        "  def up_one_dir(path):\n",
        "      try:\n",
        "          # from Python 3.6\n",
        "          parent_dir = Path(path).parents[1]\n",
        "          # for Python 3.4/3.5, use str to convert the path to string\n",
        "          # parent_dir = str(Path(path).parents[1])\n",
        "          shutil.move(path, parent_dir)\n",
        "      except IndexError:\n",
        "          # no upper directory\n",
        "          pass\n",
        "\n",
        "      \n",
        "  subdir=[path for path in os.scandir(\"D:\\Dataset1\") if path.is_dir()]\n",
        "  for i in range(1,len(subdir)+1):\n",
        "      up_one_dir(r\"D:\\Dataset1\\sub-%02d\\anat\\sub-%02d_T1w.nii.gz\" % (i, i))\n",
        "      up_one_dir(r\"D:\\Dataset1\\sub-%02d\\anat\\sub-%02d_T2w.nii.gz\" % (i, i))\n",
        "      \n",
        "  subdir=[path for path in os.scandir(\"D:\\Dataset2\") if path.is_dir()]\n",
        "  for i in range(1,len(subdir)+1):\n",
        "      up_one_dir(r\"D:\\Dataset2\\sub-%02d\\anat\\sub-%02d_T1w.nii.gz\" % (i, i))\n",
        "      up_one_dir(r\"D:\\Dataset2\\sub-%02d\\anat\\sub-%02d_T2w.nii.gz\" % (i, i))\n",
        "\n",
        "  subdir=[path for path in os.scandir(\"D:\\Dataset2\") if path.is_dir()]\n",
        "  for i in range(1,len(subdir)+1):\n",
        "      up_one_dir(r\"D:\\Dataset2\\sub-%02d\\anat\\sub-%02d_T1w.json\" % (i, i))\n",
        "      up_one_dir(r\"D:\\Dataset2\\sub-%02d\\anat\\sub-%02d_T2w.json\" % (i, i))\n",
        "\n",
        "\n",
        "\n",
        "  ######################### unzip\n",
        "  def Unzip(source_dir,dest_dir):\n",
        "      for src_name in glob.glob(os.path.join(source_dir, '*.gz')):\n",
        "          base = os.path.basename(src_name)\n",
        "          dest_name = os.path.join(dest_dir, base[:-3])\n",
        "          with gzip.open(src_name, 'rb') as infile:\n",
        "              with open(dest_name, 'wb') as outfile:\n",
        "                  for line in infile:\n",
        "                      outfile.write(line)\n",
        "                      \n",
        "  subdir=[path for path in os.scandir(\"D:\\Dataset1\") if path.is_dir()]                    \n",
        "  for i in range(1,len(subdir)+1):\n",
        "    Unzip(\"D:\\Dataset1\\sub-%02d\" % (i),\"D:\\Dataset1\\sub-%02d\" % (i))\n",
        "\n",
        "  subdir=[path for path in os.scandir(\"D:\\Dataset2\") if path.is_dir()]                    \n",
        "  for i in range(1,len(subdir)+1):\n",
        "    Unzip(\"D:\\Dataset2\\sub-%02d\" % (i),\"D:\\Dataset2\\sub-%02d\" % (i))\n",
        "\n",
        "\n",
        "  subdir=[path for path in os.scandir(\"D:\\Dataset3\") if path.is_dir()]                    \n",
        "  for i in range(1,len(subdir)+1): \n",
        "    os.rename(\"D:\\Dataset3\\BraTS20_Training_%03d\"%(i), \"D:\\Dataset3\\sub-%03d\"%(i))\n",
        "\n",
        "  subdir=[path for path in os.scandir(\"D:\\Dataset3\") if path.is_dir()]                    \n",
        "  for i in range(1,len(subdir)+1): \n",
        "    os.rename(\"D:\\Dataset3\\sub-%03d\\BraTS20_Training_%03d_t1.nii\"%(i,i), \"D:\\Dataset3\\sub-%03d\\sub-%03d_T1w.nii\"%(i,i))\n",
        "    os.rename(\"D:\\Dataset3\\sub-%03d\\BraTS20_Training_%03d_t2.nii\"%(i,i), \"D:\\Dataset3\\sub-%03d\\sub-%03d_T2w.nii\"%(i,i)) \n",
        "\n",
        "  dataset1=pd.read_csv(\"D:\\Dataset1\\participants.tsv\",sep='\\t',usecols=[\"participant_id\",\"age\",\"sex\"])\n",
        "  dataset1[\"Dataset\"]=\"D1\"\n",
        "  d = {\"female\": \"F\", \"male\": \"M\",\"Other\":\"NaN\"}\n",
        "  dataset1.replace({\"sex\": d},inplace=True)\n",
        "  dataset1[\"age\"]=dataset1[\"age\"].astype(\"int\")\n",
        "\n",
        "  dataset2=pd.read_csv(\"D:\\Dataset2\\participants.tsv\",sep='\\t',usecols=[\"participant_id\",\"age\",\"sex\"])\n",
        "  dataset2[\"Dataset\"]=\"D2\"\n",
        "  dataset2[\"age\"]=dataset2[\"age\"].astype(\"int\")\n",
        "\n",
        "\n",
        "  dataset3=pd.read_csv(\"D:\\Dataset3\\survival_info.csv\",usecols=[\"Brats20ID\",\"Age\"])\n",
        "  dataset3.rename(columns={\"Brats20ID\": \"participant_id\", \"Age\": \"age\"},inplace=True)\n",
        "  dataset3[\"Dataset\"]=\"D3\"\n",
        "  dataset3[\"sex\"]=\"NaN\"\n",
        "  dataset3[\"participant_id\"]=[\"sub-\"+i.split(\"_\")[2] for i in dataset3.participant_id]\n",
        "  dataset3[\"age\"]=dataset3[\"age\"].astype(\"int\")\n",
        "\n",
        "\n",
        "  dataset=pd.concat([dataset1,dataset3,dataset3])\n",
        "  dataset.to_csv(\"D:\\Dataset.csv\",index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mlc-199bb6nw"
      },
      "source": [
        "## Get data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hU_VrWOXcSGL"
      },
      "outputs": [],
      "source": [
        "getdata=False\n",
        "if getdata:\n",
        "  T1=[\"/content/drive/MyDrive/TFM/Dataset%d/**/*_T1w.nii\"%(i) for i in range(3,4)]\n",
        "  T2=[\"/content/drive/MyDrive/TFM/Dataset%d/**/*_T2w.nii\"%(i) for i in range(3,4)]\n",
        "  bound=[(135,309),(110,260),(30,140)]\n",
        "\n",
        "  Root_T1=\"/content/drive/MyDrive/TFM/T1\"\n",
        "  Root_T2=\"/content/drive/MyDrive/TFM/T2\"\n",
        "\n",
        "  for t1,t2,b in zip(T1,T2,bound):\n",
        "      T1_dir=glob.glob(t1,recursive=True)\n",
        "      T2_dir=glob.glob(t2,recursive=True)\n",
        "      \n",
        "      if  \"Dataset3\" in t1:\n",
        "          for t1_dir,t2_dir in zip(T1_dir,T2_dir):\n",
        "              T1_load=nib.load(t1_dir)\n",
        "              T2_load=nib.load(t2_dir)\n",
        "              s=int(re.search('\\d+',Path(t1_dir).parent.name).group(0))\n",
        "              for i in range(b[0],b[1]):\n",
        "                  np.save(os.path.join(Root_T1,\"D3_%03d_%03d_T1\"%(s,i)), T1_load.dataobj[:,:,i].astype(float))\n",
        "                  np.save(os.path.join(Root_T2,\"D3_%03d_%03d_T2\"%(s,i)), T2_load.dataobj[:,:,i].astype(float))\n",
        "              print(\"Subject {}\".format(s))\n",
        "\n",
        "      elif \"Dataset1\" in t1:\n",
        "          for t1_dir,t2_dir in zip(T1_dir,T2_dir):\n",
        "              T1_load=nib.load(t1_dir)\n",
        "              T2_load=nib.load(t2_dir)\n",
        "              s=int(re.search('\\d+',Path(t1_dir).parent.name).group(0))\n",
        "              for i in range(b[0],b[1]):\n",
        "                  np.save(os.path.join(Root_T1,\"D1_%03d_%03d_T1\"%(s,i)), T1_load.dataobj[:,i,:].astype(float))\n",
        "                  np.save(os.path.join(Root_T2,\"D1_%03d_%03d_T2\"%(s,i)), T2_load.dataobj[:,i,:].astype(float))  \n",
        "              \n",
        "      else:\n",
        "          for t1_dir,t2_dir in zip(T1_dir,T2_dir):\n",
        "              T1_load=nib.load(t1_dir)\n",
        "              T2_load=nib.load(t2_dir)\n",
        "              s=int(re.search('\\d+',Path(t1_dir).parent.name).group(0))\n",
        "              for i in range(b[0],b[1]):\n",
        "                  np.save(os.path.join(Root_T1,\"D2_%03d_%03d_T1\"%(s,i)), T1_load.dataobj[:,i,:].astype(float))\n",
        "                  np.save(os.path.join(Root_T2,\"D2_%03d_%03d_T2\"%(s,i)), T2_load.dataobj[:,i,:].astype(float))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZzPAw6d3D2X"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_tsRjAI33D2X"
      },
      "outputs": [],
      "source": [
        "def HistFunc(axis,filePath,color=None,alpha=0.05):\n",
        "    \n",
        "    data=nib.load(filePath).get_fdata()\n",
        "    IdxSample=np.random.randint(data.shape[-1], size=int(data.shape[-1]*0.6))\n",
        "    dataS=data[:,:,IdxSample]\n",
        "    dataSR=dataS.reshape(dataS.shape[::-1])\n",
        "    \n",
        "    func=lambda x: cv2.resize(x, dsize=(100, 100), interpolation=cv2.INTER_NEAREST)\n",
        "    values=np.ravel(np.array(list(map(func, dataSR))))\n",
        "    \n",
        "    kernel = stats.gaussian_kde(values)\n",
        "    positions = np.linspace(values.min(), values.max(), num=100)\n",
        "    histogram = kernel(positions)\n",
        "    \n",
        "    kwargs = dict(linewidth=1, color='black' if color is None else color, alpha=alpha)\n",
        "    axis.plot(positions,histogram, **kwargs)\n",
        "\n",
        "\n",
        "def PlotHist(directoty,axis,xix=True):\n",
        "    color=None\n",
        "    if xix==True:\n",
        "        if 'HH' in directoty: color = 'red'\n",
        "        elif 'Guys' in directoty: color = 'green'\n",
        "        elif 'IOP' in directoty: color = 'blue'\n",
        "\n",
        "    HistFunc(axis, directoty, color=color)\n",
        "    \n",
        "def Plotpdf(paths,axis,xix=True):\n",
        "    IdxSample=np.random.randint(len(paths), size=int(len(paths)*0.6))\n",
        "    Down_paths=list(map(lambda i: paths[i], IdxSample))\n",
        "    for path in tqdm(Down_paths):\n",
        "        PlotHist(path,axis,xix=True)\n",
        "        \n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def make_square(im):\n",
        "  mn=min(im.shape[0],im.shape[1])\n",
        "  new_im=Image.fromarray(im)\n",
        "  new_im=new_im.resize((mn,mn))\n",
        "  return np.array(new_im, dtype=float)\n",
        "\n",
        "\n",
        "def preparedata(Paths,balance=True):\n",
        "    #######Get images directory\n",
        "    DataObject=getDir(Paths)\n",
        "    df=DataObject.df\n",
        "\n",
        "    if balance:\n",
        "      drop_indices = np.random.choice(df[df[\"Group\"]==\"D3\"].index, DataObject.D3-(DataObject.D1+DataObject.D2), replace=False)\n",
        "      df_subset = df.drop(drop_indices).reset_index(drop=True)\n",
        "      print(\"subset is {}\".format(df_subset.shape))\n",
        "    else:\n",
        "      df_subset=df.copy()\n",
        "\n",
        "    TrainDir,tmp=train_test_split(df_subset, test_size=0.20,shuffle=True)\n",
        "    TestDir,ValtDir=train_test_split(tmp, test_size=0.50,shuffle=True)\n",
        "    \n",
        "    TrainDir=TrainDir.reset_index(drop=True)\n",
        "    ValtDir=ValtDir.reset_index(drop=True)\n",
        "    TestDir=TestDir.reset_index(drop=True)\n",
        "    print(\"Train is {}\".format(TrainDir.shape))\n",
        "    return TrainDir,ValtDir,TestDir\n",
        "\n",
        "seed_everything(seed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62fJpoMw3D2Y"
      },
      "source": [
        "## Define Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XA9kR_c53D2Y"
      },
      "outputs": [],
      "source": [
        "class getDir():\n",
        "  def __init__(self, Paths):\n",
        "    #################### Obtener directorios ####################\n",
        "    self.T1Root=Paths[\"T1\"]\n",
        "    self.T2Root=Paths[\"T2\"]\n",
        "\n",
        "    self.T1files=glob.glob(Paths[\"T1\"]+\"/*\",recursive=True)\n",
        "    self.T2files=glob.glob(Paths[\"T2\"]+\"/*\",recursive=True)\n",
        "    self.df=self.getFile()\n",
        "\n",
        "\n",
        "  def getFile(self):\n",
        "    self.T1files.sort()\n",
        "    self.T2files.sort()\n",
        "\n",
        "    list2df=[(os.path.basename(t1).split(\"_\")[0],t1,t2) for t1,t2 in zip(self.T1files,self.T2files)]\n",
        "    df=pd.DataFrame(list2df,columns=[\"Group\",\"T1\",\"T2\"])\n",
        "    self.size=df.shape[0]\n",
        "    self.D1=df[df.Group == 'D1'].shape[0]\n",
        "    self.D2=df[df.Group == 'D2'].shape[0]\n",
        "    self.D3=df[df.Group == 'D3'].shape[0]\n",
        "\n",
        "    return df\n",
        "\n",
        "################################################################################\n",
        "    \n",
        "    \n",
        "class ImageTransform:\n",
        "  def __init__(self, img_size=256):\n",
        "    self.transform = {\n",
        "        'train':  transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.Normalize(mean=[0.5], std=[0.5])]),\n",
        "        'test': transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.Normalize(mean=[0.5], std=[0.5])])}\n",
        "\n",
        "  def __call__(self, img, phase='train'):\n",
        "    img = self.transform[phase](img)\n",
        "    return img\n",
        "    \n",
        "    \n",
        "################################################################################\n",
        "\n",
        "class MRIDataset(Dataset):\n",
        "  def __init__(self, dirc, transform, phase='train',factor=1):\n",
        "    self.dirc = dirc\n",
        "    self.transform = transform\n",
        "    self.phase = phase \n",
        "    self.factor=factor       \n",
        "\n",
        "  def __len__(self):\n",
        "    return  int(self.dirc.shape[0]*self.factor)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      \n",
        "    T1_img=np.load(self.dirc.T1[idx])\n",
        "    T2_img=np.load(self.dirc.T2[idx])\n",
        "\n",
        "    if T1_img.shape[0]!=T1_img.shape[1]:\n",
        "      T1_imgN=make_square(T1_img)\n",
        "      T2_imgN=make_square(T2_img)\n",
        "    else:\n",
        "      T1_imgN=T1_img\n",
        "      T2_imgN=T2_img\n",
        "    \n",
        "    T1_img_T = self.transform(T1_imgN, self.phase)\n",
        "    T2_img_T = self.transform(T2_imgN, self.phase)\n",
        "\n",
        "    return T1_img_T.type(torch.float32),T2_img_T.type(torch.float32)\n",
        "    \n",
        "################################################################################\n",
        "    \n",
        "class MRIDatamodule(pl.LightningDataModule):\n",
        "  def __init__(self,Paths,im_size=256,batch_size=75,factor=1):\n",
        "    super(MRIDatamodule, self).__init__()\n",
        "    #self.save_hyperparameters()\n",
        "    #Define required parameters here\n",
        "    self.batch_size=batch_size\n",
        "    self.transform=ImageTransform(img_size=img_size)\n",
        "    self.factor=factor\n",
        "    self.TrainDir,self.ValtDir,self.TestDir=self.preparedata(Paths)\n",
        "    #self._log_hyperparams =None\n",
        "    self.prepare_data_per_node=False\n",
        "    #self.allow_zero_length_dataloader_with_multiple_devices=True\n",
        "\n",
        "\n",
        "  def preparedata(self,Paths,balance=True):\n",
        "    #######Get images directory\n",
        "    DataObject=getDir(Paths)\n",
        "    df=DataObject.df\n",
        "\n",
        "    if balance:\n",
        "      drop_indices = np.random.choice(df[df[\"Group\"]==\"D3\"].index, DataObject.D3-(DataObject.D1+DataObject.D2), replace=False)\n",
        "      df_subset = df.drop(drop_indices).reset_index(drop=True)\n",
        "      print(\"subset is {}\".format(df_subset.shape))\n",
        "    else:\n",
        "      df_subset=df.copy()\n",
        "\n",
        "    TrainDir,tmp=train_test_split(df_subset, test_size=0.20,shuffle=True)\n",
        "    TestDir,ValtDir=train_test_split(tmp, test_size=0.50,shuffle=True)\n",
        "    \n",
        "    TrainDir=TrainDir.reset_index(drop=True)\n",
        "    ValtDir=ValtDir.reset_index(drop=True)\n",
        "    TestDir=TestDir.reset_index(drop=True)\n",
        "    print(\"Train is {}\".format(TrainDir.shape))\n",
        "    return TrainDir,ValtDir,TestDir \n",
        "\n",
        "  def prepare_data(self):\n",
        "    \"\"\"\n",
        "    Empty prepare_data method left in intentionally. \n",
        "    https://pytorch-lightning.readthedocs.io/en/latest/data/datamodule.html#prepare-data\n",
        "    \"\"\"\n",
        "    pass             \n",
        "\n",
        "\n",
        "  def setup(self, stage=None):\n",
        "    if stage == \"fit\" or stage is None:\n",
        "      self.Train_dataset = MRIDataset(self.TrainDir,self.transform,factor=self.factor)\n",
        "      self.Val_dataset = MRIDataset(self.ValtDir, self.transform,factor=self.factor)\n",
        "    \n",
        "    if stage==\"test\":\n",
        "      self.Test_dataset = MRIDataset(self.TestDir, self.transform, phase=\"test\")\n",
        "  \n",
        "  def train_dataloader(self):\n",
        "    #print(self.Train_dataset)\n",
        "    return DataLoader(self.Train_dataset,shuffle=True,batch_size=self.batch_size)\n",
        "    \n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(self.Val_dataset,batch_size=self.batch_size)\n",
        "    \n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(self.Test_dataset,batch_size=self.batch_size)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeVlWM-hr-qN"
      },
      "source": [
        "## Modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bRVuSZdGtPzu"
      },
      "outputs": [],
      "source": [
        "class ResBlock(nn.Module):\n",
        "  '''\n",
        "  ResBlock Class:\n",
        "  @Based on the paper: \n",
        "  - [1] Jun-Yan Zhu*, Taesung Park*, Phillip Isola, and Alexei A. Efros.\n",
        "    \"Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks\",\n",
        "    in IEEE International Conference on Computer Vision (ICCV), 2017\n",
        "  - [2] Dar, S. U., Yurt, M., Karacan, L., Erdem, A., Erdem, E., & Cukur, T. (2019). \n",
        "    Image synthesis in multi-contrast MRI with conditional generative adversarial networks.\n",
        "    IEEE transactions on medical imaging, 38(10), 2375-2388.\n",
        "  - [3] https://towardsdatascience.com/residual-network-implementing-resnet-a7da63c7b278\n",
        "    \n",
        "\n",
        "  @Description\n",
        "    This class implement a Residual block, to use later in class Generator. \n",
        "    As reference one describes, residual blocks contains 2 convolutional layer\n",
        "    with a intance normalization interspersed. Finally, to achive the residual\n",
        "    efect, the output is added to the input.\n",
        "\n",
        "  @Inputs\n",
        "    Ich: Input channel\n",
        "    k_size. Kernel size Default 3, as it is defined in [1]\n",
        "    p. Padding mode as default 1,  as it is defined in [1]\n",
        "    p_m. Padding mode as 'reflect' by default,\n",
        "    dropOut=None\n",
        "\n",
        "\n",
        "  @Outputs\n",
        "    Returns the output of the block Original input+residual   \n",
        "  '''\n",
        "  \n",
        "  def __init__(self,Ich,k_size=3,p=1,p_m='reflect',dropOut=None):\n",
        "    super(ResBlock, self).__init__()\n",
        "\n",
        "    ######################  Define the block ######################\n",
        "    self.Resblock=nn.Sequential()\n",
        "    self.Resblock.add_module(\"conv1\",nn.Conv2d(Ich,Ich,kernel_size=k_size,padding=p,padding_mode=p_m))\n",
        "    self.Resblock.add_module(\"Inst_1\",nn.InstanceNorm2d(Ich))\n",
        "    self.Resblock.add_module(\"Relu_1\",nn.ReLU())\n",
        "    if dropOut: self.Resblock.add_module(\"Drop\",nn.Dropout(dropOut))\n",
        "    self.Resblock.add_module(\"conv2\",nn.Conv2d(Ich,Ich,kernel_size=k_size,padding=p,padding_mode=p_m))\n",
        "    self.Resblock.add_module(\"Inst_2\",nn.InstanceNorm2d(Ich))\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "        x: image tensor of shape (batch size, channels, height, width)\n",
        "    '''\n",
        "    original_x = x.clone()\n",
        "    x = self.Resblock(x)\n",
        "    return original_x + x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  '''\n",
        "  ResBlock Class:\n",
        "  @Based on the paper: \n",
        "  - [1] Jun-Yan Zhu*, Taesung Park*, Phillip Isola, and Alexei A. Efros.\n",
        "    \"Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks\",\n",
        "    in IEEE International Conference on Computer Vision (ICCV), 2017\n",
        "  - [2] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... & Bengio, Y. (2020).\n",
        "    Generative adversarial networks. Communications of the ACM, 63(11), 139-144.\n",
        "  - [3] Dar, S. U., Yurt, M., Karacan, L., Erdem, A., Erdem, E., & Cukur, T. (2019). \n",
        "    Image synthesis in multi-contrast MRI with conditional generative adversarial networks.\n",
        "    IEEE transactions on medical imaging, 38(10), 2375-2388.\n",
        "  - [4] https://lightning.ai/docs/pytorch/stable/notebooks/lightning_examples/basic-gan.html\n",
        "    \n",
        "\n",
        "  @Description\n",
        "    Following references [1,2] a generator with 9 residual blocks consists is created\n",
        "    to use the cyclegan on images of 256x26. However this is the standard model. The \n",
        "    sript is open to add more residual blocks or change input parameters such as kernel,\n",
        "    padding.\n",
        "    \n",
        "    The default layers are:\n",
        "    c7s1-64,d128,d256,R256,R256,R256,R256,R256,R256,R256,R256,R256,u128,u64,c7s1-3\n",
        "\n",
        "    Notation example:\n",
        "    - c7s1-k denote a7×7 Convolution-Instance Norm-ReLU layer with k filters and stride 1. \n",
        "    - dk denotes a3×3 Convolution-InstanceNorm-ReLU layer  with k filters  and stride 2.\n",
        "    - Rk denotes a residual block that contains two 3×3 convolutional layers with the same number of filters on both layer.\n",
        "    - uk denotes a 3×3 fractional-strided-Convolution-InstanceNorm-ReLU layer with k filters and stride 1/2.\n",
        "\n",
        "  @Inputs\n",
        "    in_f: input (image)\n",
        "    out_f. by default 64 (int), number of filters in first layer. Take into account the input size and the encoder leverls\n",
        "    lvl_aut: Autoencoder levels, by default 2. Since encoder and deconder has same levels, this is a int with the levels of only one of them\n",
        "    lvl_resnt: Resinual neck levels,  by default 9.\n",
        "\n",
        "\n",
        "\n",
        "  @Outputs\n",
        "    Returns the output of the block Original input+residual   \n",
        "  '''\n",
        "  \n",
        "  def __init__(self,in_f,out_f=64,lvl_aut=2,lvl_resnt=9,upsam=None):\n",
        "    super(Generator, self).__init__()\n",
        "\n",
        "    ######################  Define constants and variables ######################\n",
        "    f_deep=[2**i for i in range(1,lvl_aut+1)] #level scale\n",
        "    rInput=out_f*(2**lvl_aut) # Input size Rest blocks\n",
        "    self.gen=nn.Sequential()\n",
        "\n",
        "    ######################  Define the encoder ######################\n",
        "    self.gen.add_module(f\"c7s1_{out_f}\", self.EncLayer(in_f,out_f,nor=False,k=7,p=3,s=1,act='relu'))#c7s1-64\n",
        "\n",
        "    #d128,d256\n",
        "    Ich=out_f\n",
        "    for f in f_deep:\n",
        "      self.gen.add_module(f\"d{out_f*f}\", self.EncLayer(Ich,out_f*f,nor=True, k=3,p=1,s=2,act='relu'))\n",
        "      Ich=out_f*f\n",
        "\n",
        "\n",
        "    ######################  Define the residual neck ######################\n",
        "    for l in range(lvl_resnt):\n",
        "     self.gen.add_module(f\"R{rInput}_{l}\", ResBlock(rInput,k_size=3,p=1,p_m='reflect',dropOut=None))  #R256 x 9\n",
        "\n",
        "\n",
        "    ######################  Define the Decoder ######################\n",
        "    #u128,u64\n",
        "    Ich=rInput\n",
        "    for f in f_deep:\n",
        "     self.gen.add_module(f\"u{rInput//f}\", self.DecLayer(Ich,rInput//f,nor=True,k=3,s=2,p=1,op=1,act=\"relu\"))\n",
        "     Ich=rInput//f\n",
        "\n",
        "    ######################  Last layer ######################\n",
        "    #self.c7s1_3 = nn.Sequential()\n",
        "    self.gen.add_module(\"c7s1_3\", nn.Conv2d(out_f,\n",
        "                              in_f, \n",
        "                              kernel_size=7, \n",
        "                              padding=3, \n",
        "                              stride=1, \n",
        "                              padding_mode='reflect'))\n",
        "    \n",
        "    self.gen.add_module(\"Tanh\", nn.Tanh())\n",
        "\n",
        "\n",
        "  #--------------------------------------- Methods ---------------------------------------#\n",
        "  def EncLayer(self,Ich,Och,nor=True, k=3,p=1,s=2,act='relu'):\n",
        "    '''\n",
        "    @Description\n",
        "      Creates dk layer using a convoutional layer, with ot without normalizaton.\n",
        "      This is the downsampling part, in other words the layer of the encoder\n",
        "\n",
        "    @Inputs\n",
        "      Ich: Input channels, int\n",
        "      Och: Output channles, int\n",
        "      k. kernel size By default 3\n",
        "      p. Padding, by default 1\n",
        "      s. Stride, as is defined in [1] it is 2 as default\n",
        "      relu. Boolean, True as defaul uses Relu. False use nn.LeakyReLU(0.2)\n",
        "      drop.  by default is None. Float number indicates the percentage of a dropout layer\n",
        "      norm. by default is True for instance normalization. False indicates no instance normalization layer.\n",
        "\n",
        "    @Outputs\n",
        "      Sequential model wich correspond a dk layer \n",
        "    '''\n",
        "    m = nn.Sequential()\n",
        "    m.add_module(\"conv1\", nn.Conv2d(Ich, Och, kernel_size=k, padding=p, stride=s, padding_mode='reflect'))\n",
        "    if nor: m.add_module(\"Instancenorm\", nn.InstanceNorm2d(Och)) \n",
        "    m.add_module(\"activation\", nn.ReLU()) if act else m.add_module(\"activation\", nn.LeakyReLU(0.2))\n",
        "    return m\n",
        "\n",
        "  def DecLayer(self,Ich,Och,nor=True,k=3,s=2,p=1,op=1,act=\"relu\"):\n",
        "    '''\n",
        "    @Description\n",
        "      Creates uk layer using a deconvoutional layer, with or without normalization.\n",
        "      This is the upsampling part, in other words the layer of the decoder\n",
        "\n",
        "    @Inputs\n",
        "      Ich: Input channels, int\n",
        "      Och: Output channles, int\n",
        "      k_size. kernel size By default 3\n",
        "      p. Padding, by default 1\n",
        "      p_m. Padding mode, by default 'reflect'\n",
        "      s. Stride, as is defined in [1] it is 2 as default\n",
        "      relu. Boolean, True as defaul uses Relu. False use nn.LeakyReLU(0.2)\n",
        "      drop.  by default is None. Float number indicates the percentage of a dropout layer\n",
        "      norm. by default is True for instance normalization. False indicates no instance normalization layer.\n",
        "      upsampling. by default is None. int number indicates the scale factor for upsampling\n",
        "\n",
        "    @Outputs\n",
        "      Sequential model wich correspond a uk layer \n",
        "    '''\n",
        "    m = nn.Sequential()\n",
        "    m.add_module(\"dconv1\",  nn.ConvTranspose2d(Ich,Och, kernel_size=k, stride=s, padding=p, output_padding=op))\n",
        "    if nor: m.add_module(\"Instancenorm\", nn.InstanceNorm2d(Och)) \n",
        "    m.add_module(\"activation\", nn.ReLU()) if act else m.add_module(\"activation\", nn.LeakyReLU(0.2))\n",
        "    return m\n",
        "\n",
        "\n",
        "  #---------------------------------------  Call funtion ---------------------------------------#\n",
        "  def forward(self, x):\n",
        "    x=self.gen(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "  '''\n",
        "  Discriminator Class: \n",
        "  @Based on the paper: \n",
        "   - [1] Jun-Yan Zhu*, Taesung Park*, Phillip Isola, and Alexei A. Efros.\n",
        "    \"Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks\",\n",
        "    in IEEE International Conference on Computer Vision (ICCV), 2017\n",
        "   - [2] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... & Bengio, Y. (2020).\n",
        "    Generative adversarial networks. Communications of the ACM, 63(11), 139-144.\n",
        "   - [3] https://arxiv.org/abs/1703.10593\n",
        "   - [4] Dar, S. U., Yurt, M., Karacan, L., Erdem, A., Erdem, E., & Cukur, T. (2019). \n",
        "    Image synthesis in multi-contrast MRI with conditional generative adversarial networks.\n",
        "    IEEE transactions on medical imaging, 38(10), 2375-2388. \n",
        "    \n",
        "\n",
        "  @Description\n",
        "    The discriminator yields  a matrix of values classifying corresponding \n",
        "    portions of the image as real or fake.\n",
        "\n",
        "    Following references [1,2,3] for   discriminator   net-works, we use 70×70 PatchGAN.  \n",
        "    After the last layer, we apply a convolution to produce a 1-dimensional output. leaky ReLUs\n",
        "    with a slope of 0.2 is used to deal with vanishing problems. After the last layer, we apply\n",
        "    a convolution to produce a 1-dimensional output\n",
        "     \n",
        "    The default layers are:\n",
        "    C64-C128-C256-C512-C1\n",
        "\n",
        "    Notation example:\n",
        "    - Ck denote a 4×4 Convolution - InstanceNorm - LeakyReLU (0.2) layer with k filters and stride 2\n",
        "\n",
        "\n",
        "  @Inputs\n",
        "    ICh: the number of image input channels\n",
        "    HCh: Hidden layers. the initial number of discriminator convolutional filters\n",
        "    n. Number of layer to implement\n",
        "\n",
        "\n",
        "  @Outputs\n",
        "    Returns patchGAN Discriminator \n",
        "  '''\n",
        "\n",
        "  def __init__(self,ICh,HCh=64,n=3):\n",
        "    super(Discriminator, self).__init__()\n",
        "\n",
        "    ######################  Define constants and variables ######################\n",
        "    f_deep=[2**i for i in range(1,n+1)] #level scale\n",
        "    self.disc=nn.Sequential()\n",
        "    self.disc.add_module(f\"C{HCh}\",self.lcreation(ICh,HCh,k_size=4,p=1,s=2,drop=None,relu=False,norm=False))\n",
        "\n",
        "    ######################  Define layers ######################\n",
        "    input=HCh\n",
        "    for f in f_deep:\n",
        "      self.disc.add_module(f\"C{HCh*f}\",self.lcreation(input,HCh*f,k_size=4,p=1,s=2,drop=None,relu=False,norm=True))\n",
        "      input=HCh*f\n",
        "\n",
        "    ######################  Define layers ######################\n",
        "    self.disc.add_module(f\"C1\",nn.Conv2d(HCh*(2**n), 1, kernel_size=4, padding=1))\n",
        "\n",
        "  #---------------------------------------  Methods ---------------------------------------#\n",
        "  def lcreation(self,Ich,Och,k_size=4,p=1,s=2,drop=None,relu=True,norm=True):\n",
        "    '''\n",
        "    @Description\n",
        "    Creates layers according reference [1]. Using Convolution - InstanceNorm - LeakyReLU (0.2) \n",
        "    if it is need it.\n",
        "\n",
        "    @Inputs\n",
        "     ICh: the number of image input channels\n",
        "     Och: Number of filters in the conv layer\n",
        "     k_size. Kernal size, by default 4\n",
        "     p. Padding by default 1\n",
        "     s stride by default 2\n",
        "     drop dropout factor, by default None\n",
        "     relu if apply relu o leaky. By default Relu using True\n",
        "     norm. Apply instance normalization. By defaul True\n",
        "\n",
        "    @Outputs\n",
        "      Returns sequential model.\n",
        "    '''\n",
        "\n",
        "    m = nn.Sequential()\n",
        "    m.add_module(\"conv1\", nn.Conv2d(Ich, \n",
        "                              Och, \n",
        "                              kernel_size=k_size, \n",
        "                              padding=p, \n",
        "                              stride=s))\n",
        "    \n",
        "    if norm: m.add_module(\"Instancenorm\", nn.InstanceNorm2d(Och)) \n",
        "    m.add_module(\"activation\", nn.ReLU()) if relu else m.add_module(\"activation\", nn.LeakyReLU(0.2))\n",
        "    if drop: m.add_module(\"Dropout\", nn.Dropout(drop)) \n",
        "    return m\n",
        "  #---------------------------------------  Call funtion ---------------------------------------#\n",
        "  def forward(self, x):\n",
        "    x=self.disc(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lgimWMUkdUH8"
      },
      "outputs": [],
      "source": [
        "class CycleGAN(pl.LightningModule):\n",
        "  '''\n",
        "  CycleGAN Class: \n",
        "  @Based on the paper: \n",
        "   - [1] Jun-Yan Zhu*, Taesung Park*, Phillip Isola, and Alexei A. Efros.\n",
        "    \"Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks\",\n",
        "    in IEEE International Conference on Computer Vision (ICCV), 2017\n",
        "   - [2] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... & Bengio, Y. (2020).\n",
        "    Generative adversarial networks. Communications of the ACM, 63(11), 139-144.\n",
        "   - [3] https://arxiv.org/abs/1703.10593\n",
        "   - [4] Dar, S. U., Yurt, M., Karacan, L., Erdem, A., Erdem, E., & Cukur, T. (2019). \n",
        "    Image synthesis in multi-contrast MRI with conditional generative adversarial networks.\n",
        "    IEEE transactions on medical imaging, 38(10), 2375-2388.\n",
        "   - [5] https://lightning.ai/docs/pytorch/stable/notebooks/lightning_examples/basic-gan.html\n",
        "   - [6] https://www.assemblyai.com/blog/pytorch-lightning-for-dummies/\n",
        "    \n",
        "\n",
        "  @Description\n",
        "  @Inputs\n",
        "  @Outputs\n",
        "  '''\n",
        "  def __init__(self,\n",
        "               input,\n",
        "               params,\n",
        "               features=64):\n",
        "    super(CycleGAN,self).__init__()\n",
        "\n",
        "    ############# Customize class #############\n",
        "    self.save_hyperparameters(params)\n",
        "    self.automatic_optimization = False\n",
        "    self.target_shape=target_shape\n",
        "    #self.device=\"cuda\"\n",
        "    self.lr = params[\"lr\"]   \n",
        "    self.b1 = params[\"b1\"]\n",
        "    self.b2 = params[\"b2\"]\n",
        "    self.lbc_T1 = params[\"lbc_T1\"]   \n",
        "    self.lbc_T2 = params[\"lbc_T2\"]\n",
        "    self.btch_size = params[\"batch_size\"]\n",
        "    self.target_shape = params[\"target_shape\"]\n",
        "    self.lbi=params[\"lbi\"]\n",
        "\n",
        "    ############# Define components #############\n",
        "    self.G_T1_T2=Generator(input,out_f=features,lvl_aut=2,lvl_resnt=9)\n",
        "    self.D_T1=Discriminator(input,HCh=features,n=3)\n",
        "\n",
        "    self.G_T2_T1=Generator(input,out_f=features,lvl_aut=2,lvl_resnt=9)\n",
        "    self.D_T2=Discriminator(input,HCh=features,n=3)\n",
        "\n",
        "\n",
        "    self.G_T1_T2=self.G_T1_T2.apply(self.weights_init)\n",
        "    self.D_T1=self.D_T1.apply(self.weights_init)\n",
        "    self.G_T2_T1=self.G_T2_T1.apply(self.weights_init)\n",
        "    self.D_T2=self.D_T2.apply(self.weights_init)\n",
        "\n",
        "\n",
        "\n",
        "   ############# Define loss #############\n",
        "    self.identity_loss = torch.nn.L1Loss()\n",
        "    self.adv_loss = torch.nn.MSELoss() #adversarial loss function to keep track of how well the GAN is fooling the discriminator and how well the discriminator is catching the GAN\n",
        "    self.cycle_loss = torch.nn.L1Loss()\n",
        "    \n",
        "\n",
        "  def forward(self, x):\n",
        "    x=self.G_T1_T2(x)\n",
        "    return x\n",
        "\n",
        "  def training_step(self, batch):\n",
        "    '''\n",
        "      @Description:\n",
        "      - [1] https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\n",
        "      Compute loss accorgin original implementation of CycleGAN.\n",
        "    \n",
        "\n",
        "      @Inputs\n",
        "\n",
        "      @Outputs\n",
        "    '''\n",
        "    ############# Initialization #############\n",
        "    real_T1, real_T2 = batch\n",
        "    Gopt,Dopt_T1,Dopt_T2=self.configure_optimizers()\n",
        "\n",
        "    ############# update discriminator #############\n",
        "    #### Discriminator T1\n",
        "    self.toggle_optimizer(Dopt_T1)\n",
        "   \n",
        "    f_T1 = self.G_T2_T1(real_T2)\n",
        "    T1Loss_Dics = self.DiscLoss(real_T1,f_T1,disc=\"T1\")\n",
        "    \n",
        "    self.manual_backward(T1Loss_Dics,retain_graph=True)\n",
        "    Dopt_T1.step()\n",
        "    Dopt_T1.zero_grad() # Zero out the gradient before backpropagation\n",
        "    self.untoggle_optimizer(Dopt_T1)\n",
        "\n",
        "    #### Discriminator T2\n",
        "    self.toggle_optimizer(Dopt_T2)\n",
        "    \n",
        "    f_T2 = self.G_T1_T2(real_T1)\n",
        "    T2Loss_Dics = self.DiscLoss(real_T2,f_T2,disc=\"T2\")\n",
        "    \n",
        "    self.manual_backward(T2Loss_Dics,retain_graph=True)\n",
        "    Dopt_T2.step()\n",
        "    Dopt_T2.zero_grad() # Zero out the gradient before backpropagation\n",
        "    self.untoggle_optimizer(Dopt_T2)\n",
        "\n",
        "    ############# update Generator #############\n",
        "    self.toggle_optimizer(Gopt)\n",
        "    gen_loss, f_T1, f_T2,Iden_term,Cycle_term,Adv_term = self.GenLoss(real_T1, real_T2)\n",
        "    \n",
        "    self.manual_backward(gen_loss) # Update gradients\n",
        "    Gopt.step() # Update optimizer\n",
        "    Gopt.zero_grad()\n",
        "    self.untoggle_optimizer(Gopt)\n",
        "    \n",
        "    ############# Compute Training metrics #############\n",
        "    #G_psnr_T2,G_ssim_T2,G_psnr_T1,G_ssim_T1=ComputeMetrics(f_T2, real_T2,f_T1, real_T1)\n",
        "\n",
        "\n",
        "    ########### Loggers ###########\n",
        "    self.log(\"D_loss_T1\", T1Loss_Dics, prog_bar=True)\n",
        "    self.log(\"D_loss_T2\", T2Loss_Dics, prog_bar=True)\n",
        "    self.log(\"G_loss\", gen_loss, prog_bar=True)\n",
        "    #self.log(\"G_psnr_T2\", G_psnr_T2)\n",
        "    #self.log(\"G_ssim_T2\", G_ssim_T2)\n",
        "    #self.log(\"G_psnr_T1\", G_psnr_T1)\n",
        "    #self.log(\"G_ssim_T1\", G_ssim_T1)\n",
        "\n",
        "\n",
        "    #Loss\n",
        "    loss={'G_loss': gen_loss, \n",
        "          'D_loss_T2': T2Loss_Dics, \n",
        "          'D_loss_T1': T1Loss_Dics, \n",
        "          'identity': Iden_term,\n",
        "          'Cycle_term': Cycle_term, \n",
        "          \"Adver_term\":Adv_term}#,\n",
        "          #\"G_psnr_T2\": G_psnr_T2,\n",
        "          #\"G_ssim_T2\": G_ssim_T2,\n",
        "          #\"G_psnr_T1\": G_psnr_T1,\n",
        "          #\"G_ssim_T1\": G_ssim_T1}\n",
        "        \n",
        "    return loss\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "\n",
        "      ############# Initialization #############\n",
        "      real_T1, real_T2 = batch\n",
        "      Gopt,Dopt_T1,Dopt_T2=self.configure_optimizers()\n",
        "      \n",
        "      ############# update discriminator #############\n",
        "      #### Discriminator T1\n",
        "      f_T1 = self.G_T2_T1(real_T2)\n",
        "      T1Loss_Dics = self.DiscLoss(real_T1,f_T1,disc=\"T1\")\n",
        "   \n",
        "      #### Discriminator T2\n",
        "      f_T2 = self.G_T1_T2(real_T1)\n",
        "      T2Loss_Dics = self.DiscLoss(real_T2,f_T2,disc=\"T2\")\n",
        "\n",
        "      ############# update Generator #############\n",
        "      gen_loss, f_T1, f_T2,Iden_term,Cycle_term,Adv_term = self.GenLoss(real_T1, real_T2)\n",
        "\n",
        "      ############# Compute Training metrics #############\n",
        "      G_psnr_T2,G_ssim_T2,G_psnr_T1,G_ssim_T1,Vfid_T1,Vfid_T2=self.ComputeMetrics(f_T2, real_T2,f_T1, real_T1)\n",
        "\n",
        "      ########### Loggers ###########\n",
        "      self.log(\"Dval_loss_T1\", T1Loss_Dics, prog_bar=True)\n",
        "      self.log(\"Dval_loss_T2\", T2Loss_Dics, prog_bar=True)\n",
        "      self.log(\"Gval_loss\", gen_loss, prog_bar=True)\n",
        "      self.log(\"Gval_psnr_T2\", G_psnr_T2)\n",
        "      self.log(\"Gval_ssim_T2\", G_ssim_T2)\n",
        "      self.log(\"Gval_psnr_T1\", G_psnr_T1)\n",
        "      self.log(\"Gval_ssim_T1\", G_ssim_T1)\n",
        "      self.log(\"Vfid_T1\", Vfid_T1)\n",
        "      self.log(\"Vfid_T2\", Vfid_T2)\n",
        "\n",
        "      loss= {'Gval_loss': gen_loss,\n",
        "             'Dval_loss_T2': T2Loss_Dics,\n",
        "             'Dval_loss_T1': T1Loss_Dics,\n",
        "             'Val_identity': Iden_term,\n",
        "             'Val_Cycle_term': Cycle_term,\n",
        "             \"Val_Adver_term\":Adv_term,\n",
        "             \"Gval_psnr_T2\": G_psnr_T2,\n",
        "             \"Gval_ssim_T2\": G_ssim_T2,\n",
        "             \"Gval_psnr_T1\": G_psnr_T1,\n",
        "             \"Gval_ssim_T1\": G_ssim_T1,\n",
        "             \"Vfid_T1\": Vfid_T1,\n",
        "             \"Vfid_T2\": Vfid_T2}\n",
        "\n",
        "\n",
        "      return loss\n",
        "  \n",
        "  def configure_optimizers(self):\n",
        "    '''\n",
        "    @Description:\n",
        "    - [1] https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\n",
        "\n",
        "    Inicialize the optimizers. As it describe in original CycleGAN implementation [1]\n",
        "    the optimizers are ADAMS. \n",
        "  \n",
        "\n",
        "    @Inputs\n",
        "\n",
        "    @Outputs\n",
        "    '''\n",
        "\n",
        "    lr = self.lr\n",
        "    b1 = self.b1\n",
        "    b2 = self.b2\n",
        "\n",
        "    #Gopt_T1_T2 = torch.optim.Adam(self.G_T1_T2.parameters(), lr=lr, betas=(b1, b2))\n",
        "    Dopt_T1= torch.optim.Adam(self.D_T1.parameters(), lr=lr, betas=(b1, b2))\n",
        "\n",
        "    #Gopt_T2_T1 = torch.optim.Adam(self.G_T2_T1.parameters(), lr=lr, betas=(b1, b2))\n",
        "    Dopt_T2 = torch.optim.Adam(self.D_T2.parameters(), lr=lr, betas=(b1, b2))\n",
        "    Gopt= torch.optim.Adam(list(self.G_T1_T2.parameters()) + list(self.G_T2_T1.parameters()), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "    return Gopt,Dopt_T1,Dopt_T2\n",
        "  \n",
        "  def DiscLoss(self,real,fake,disc=\"T1\"):\n",
        "    '''\n",
        "    @Description\n",
        "    This function computes the discriminator loss using the adversarial loss funtion\n",
        "    MSE. Taking the target label and the discriminator predictions returns the adversarial loss.\n",
        "    With adverarial loss from real and from fake image we compute the discriminator loss such as:\n",
        "\n",
        "    discriminator loss= (adv_fake+adv_real)/2\n",
        "\n",
        "    @Inputs\n",
        "    real. Tensor, real image.\n",
        "    fake. Tensor, fake image.\n",
        "    \n",
        "    @Outputs\n",
        "    Discriminator loss\n",
        "\n",
        "    '''\n",
        "\n",
        "    if disc == \"T1\":\n",
        "      disc_fake_hat = self.D_T1(fake.detach())      \n",
        "      disc_real_hat = self.D_T1(real)\n",
        "    else:\n",
        "      disc_fake_hat = self.D_T2(fake.detach())\n",
        "      disc_real_hat = self.D_T2(real)\n",
        "\n",
        "    fake_loss = self.adv_loss(disc_fake_hat, torch.zeros_like(disc_fake_hat))\n",
        "    real_loss = self.adv_loss(disc_real_hat, torch.ones_like(disc_real_hat))\n",
        "\n",
        "    r=(fake_loss + real_loss) / 2\n",
        "    return r\n",
        "\n",
        "  def GenLoss(self, real_T1, real_T2):\n",
        "    '''\n",
        "    @Description\n",
        "    @Inputs\n",
        "    @Outputs\n",
        "    '''\n",
        "\n",
        "    #compute fakes\n",
        "    f_T1 = self.G_T2_T1(real_T2)\n",
        "    f_T2 = self.G_T1_T2(real_T1)\n",
        "\n",
        "    #Compute Discriminators output\n",
        "    dic_f_T1_hat = self.D_T1(f_T1)\n",
        "    dic_f_T2_hat = self.D_T2(f_T2)\n",
        "\n",
        "    # Compute adversarial loss AdvLoss_T2_T1 +  AdvLoss_T1_T2\n",
        "    Adv_term=self.adv_loss(dic_f_T1_hat, torch.ones_like(dic_f_T1_hat)) + self.adv_loss(dic_f_T2_hat, torch.ones_like(dic_f_T2_hat))\n",
        "\n",
        "    # Compute Cycles\n",
        "    C_T1 = self.G_T2_T1(f_T2)\n",
        "    C_T2 = self.G_T1_T2(f_T1)\n",
        "\n",
        "    # Compute Cycle consistancy. \n",
        "    Cycle_term=self.lbc_T1*self.cycle_loss(C_T1,real_T1)+self.lbc_T2*self.cycle_loss(C_T2,real_T2)\n",
        "        \n",
        "    #Compute Identities\n",
        "    identity_T1 = self.G_T2_T1(real_T1)\n",
        "    identity_T2 = self.G_T1_T2(real_T2)\n",
        "\n",
        "    # Compute Identity term\n",
        "    Iden_term =  self.identity_loss (identity_T1, real_T1) + self.identity_loss (identity_T2, real_T2)\n",
        "\n",
        "    # Compute Total loss\n",
        "    gen_loss = self.lbi * Iden_term +  Cycle_term + Adv_term\n",
        "\n",
        "\n",
        "    return gen_loss, f_T1, f_T2,Iden_term,Cycle_term,Adv_term\n",
        "\n",
        "  def weights_init(self,m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "    if isinstance(m, nn.BatchNorm2d):\n",
        "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias, 0)\n",
        "\n",
        "  def ComputeMetrics(self,f_T2, real_T2,f_T1, real_T1):\n",
        "    ############# Metrics Generator #############\n",
        "    psnr_metric = PSNR().to(self.device)\n",
        "    ssim_metric = SSIM().to(self.device)\n",
        "\n",
        "    G_psnr_T2 = psnr_metric(f_T2, real_T2)\n",
        "    G_ssim_T2 = ssim_metric(f_T2, real_T2)\n",
        "\n",
        "    G_psnr_T1 = psnr_metric(f_T1, real_T1)\n",
        "    G_ssim_T1 = ssim_metric(f_T1, real_T1)\n",
        "\n",
        "    #Compute FID with the InceptionV3 model\n",
        "    #block_idx = models.inceptionV3.BLOCK_INDEX_BY_DIM[2048]\n",
        "    #model1 = models.inceptionV3([block_idx])\n",
        "    #model1.eval()\n",
        "\n",
        "    # Compute the FID score\n",
        "    fid_T1 = 0.0 #fid_score(real_T1, f_T1, model1, device=self.device)\n",
        "    fid_T2 = 0.0 #fid_score(real_T2, f_T2, model1, device=self.device)\n",
        "\n",
        "    return G_psnr_T2,G_ssim_T2,G_psnr_T1,G_ssim_T1,fid_T1,fid_T2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Callbacks"
      ],
      "metadata": {
        "id": "YejJ-DDKEms3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Entre mas alto el SSIM mejor\n",
        "\n",
        "chk_pth=\"/content/drive/MyDrive/TFM/Checkpoints\"\n",
        "n_train_steps=1000\n",
        "patient=800\n",
        "\n",
        "early_stop_callback = EarlyStopping(\n",
        "   monitor='Gval_ssim_T2',\n",
        "   patience=patient,\n",
        "   verbose=False,\n",
        "   mode='max'\n",
        ")\n",
        "\n",
        "check = ModelCheckpoint(\n",
        "    save_top_k=1,\n",
        "    monitor=\"Gval_ssim_T2\",\n",
        "    mode=\"max\",\n",
        "    dirpath=chk_pth,\n",
        "    every_n_train_steps=n_train_steps,\n",
        "    filename=\"Model-{step:06d}{epoch:03d}-{Gval_loss:.2f}-{Val_identity:.2f}\"\n",
        ")\n",
        "\n",
        "\n",
        "class VisualizeFakeImages(pl.Callback):\n",
        "  def __init__(self, dataloader, every_n_steps=100):\n",
        "    super().__init__()\n",
        "    self.dataloader = dataloader\n",
        "    self.every_n_steps = every_n_steps\n",
        "      \n",
        "  def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
        "    # Check if this step is the right one to visualize the fake images\n",
        "    if (trainer.global_step + 1) % self.every_n_steps != 0:\n",
        "      return\n",
        "\n",
        "\n",
        "    # Generate fake images\n",
        "    T1, T2 = batch\n",
        "    with torch.no_grad():\n",
        "      f_T2 = pl_module.G_T2_T1(T1)\n",
        "    \n",
        "    # Create a grid of real and fake images\n",
        "    grid = vutils.make_grid(\n",
        "        torch.cat([T2, f_T2], dim=0),\n",
        "        normalize=True,\n",
        "        scale_each=True,\n",
        "        nrow=1\n",
        "    )\n",
        "    \n",
        "    logger = trainer.logger\n",
        "    if logger is not None:\n",
        "      logger.experiment.add_image(\n",
        "          f\"Image_Epoch{trainer.current_epoch}_Batch{batch_idx}\",\n",
        "          grid.permute(1, 2, 0).cpu().numpy(),\n",
        "          global_step=trainer.global_step\n",
        "      )\n",
        "\n",
        "    # Display the grid of images\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.imshow(grid.permute(1, 2, 0).cpu())\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "5-qrL0xqbfQR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frQ_P3gBgLMX"
      },
      "source": [
        "## Cycle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C36cZ3Zg3D2Z",
        "outputId": "d54928fa-7d89-4b9d-fa87-3fba6c9d6a1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "subset is (41268, 3)\n",
            "Train is (33014, 3)\n"
          ]
        }
      ],
      "source": [
        "#Parameters\n",
        "batch_size = 1  #When batch_size = 1, it's instance_normalization. \n",
        "#When batch_size > 1, it's batch_normalization. instance_normalization\n",
        "#is better than batch_normalization for image2image transfer.\n",
        "img_size = 256\n",
        "Paths = {\"T1\":\"/content/drive/MyDrive/TFM/T1\",\"T2\":\"/content/drive/MyDrive/TFM/T2\"}\n",
        "lr=0.0002\n",
        "lbc_T1 = 10\n",
        "lbc_T2 = 10\n",
        "lbi = 0.1\n",
        "b1 = 0.5\n",
        "b2 = 0.999\n",
        "target_shape = 256\n",
        "epochs = 4\n",
        "input=1\n",
        "\n",
        "#Get Image Dataloader\n",
        "data_mri=MRIDatamodule(Paths,im_size=img_size,batch_size=batch_size,factor=0.1)\n",
        "\n",
        "\n",
        "params = {'lr': lr,\n",
        "          'lbc_T1': lbc_T1,\n",
        "          'lbc_T2': lbc_T2,\n",
        "          'lbi': lbi,\n",
        "          'lr': 1e-05,\n",
        "          'batch_size': (0.9, 0.999),\n",
        "          'b1': b1,\n",
        "          'b2': b2,\n",
        "          'target_shape': target_shape,\n",
        "          'batch_size': batch_size}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Instance Model\n",
        "model= CycleGAN(input,params)\n",
        "\n",
        "#Instance Callbaks\n",
        "tb_logger = TensorBoardLogger(\"/content/drive/MyDrive/TFM/loggers/\", name=\"cycleGAN\")\n",
        "vis= VisualizeFakeImages(data_mri, every_n_steps=n_train_steps)\n",
        "\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=epochs,\n",
        "    accelerator=\"gpu\",\n",
        "    val_check_interval=0.5,\n",
        "    callbacks=[check,vis],\n",
        "    logger=tb_logger)\n",
        "\n",
        "#trainer = Trainer(resume_from_checkpoint='some/path/to/my_checkpoint.ckpt')\n",
        "trainer.fit(model, data_mri)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743,
          "referenced_widgets": [
            "cf8aa1c0581e4b959801d7d1bdd095c1",
            "33572b969b1b42dab2f8ef5f96d8b194",
            "7708d45207fb4eb787edfd38aaf9af93",
            "f0c0b02314664d48a4c5a8d30d1b1766",
            "9189285bebb64081808b8b72fd6c266c",
            "ca289ee35e444cd0a4d0f95c52384b9a",
            "981fe890794546009b530e73b05da56a",
            "48bf7a1b4a4c4e5abb70bef341ed8e3a",
            "ce3674b96b534b43b4d1fedf248ea1d7",
            "d318988ceff949c58df86dfe6043503c",
            "8b20e7a95ab54d48a08f83dad86d2784",
            "9c930d9037b94fc7b6fb263f46bb5253",
            "c6cacf0c9d7149f1a74a7494f241051e",
            "33dd9a051082468294e5aa259a0f2ed6",
            "a59ca0583de5443489e2f43e8de374fc",
            "76a6773957904eb4881a03a595543a6a",
            "6851889c74264d098b10bc8bb777898c",
            "24c46a5e5fe842c787d252bbc1f3c2e2",
            "6f8e1ede676b43d89563d42724741300",
            "a189de1e24014aa6995e77541b77398e",
            "3283c91a5c0042bf82edf1ac4a35f85e",
            "efe0040d2ccd46e2bf66427c372cd081"
          ]
        },
        "id": "b9zWUJPArnMu",
        "outputId": "04a53cf9-63d5-4312-8c23-daa3c2361a6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: \n",
            "  | Name          | Type          | Params\n",
            "------------------------------------------------\n",
            "0 | G_T1_T2       | Generator     | 11.4 M\n",
            "1 | D_T1          | Discriminator | 2.8 M \n",
            "2 | G_T2_T1       | Generator     | 11.4 M\n",
            "3 | D_T2          | Discriminator | 2.8 M \n",
            "4 | identity_loss | L1Loss        | 0     \n",
            "5 | adv_loss      | MSELoss       | 0     \n",
            "6 | cycle_loss    | L1Loss        | 0     \n",
            "------------------------------------------------\n",
            "28.3 M    Trainable params\n",
            "0         Non-trainable params\n",
            "28.3 M    Total params\n",
            "113.027   Total estimated model params size (MB)\n",
            "INFO:lightning.pytorch.callbacks.model_summary:\n",
            "  | Name          | Type          | Params\n",
            "------------------------------------------------\n",
            "0 | G_T1_T2       | Generator     | 11.4 M\n",
            "1 | D_T1          | Discriminator | 2.8 M \n",
            "2 | G_T2_T1       | Generator     | 11.4 M\n",
            "3 | D_T2          | Discriminator | 2.8 M \n",
            "4 | identity_loss | L1Loss        | 0     \n",
            "5 | adv_loss      | MSELoss       | 0     \n",
            "6 | cycle_loss    | L1Loss        | 0     \n",
            "------------------------------------------------\n",
            "28.3 M    Trainable params\n",
            "0         Non-trainable params\n",
            "28.3 M    Total params\n",
            "113.027   Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf8aa1c0581e4b959801d7d1bdd095c1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c930d9037b94fc7b6fb263f46bb5253"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfgWLPlsnA2i"
      },
      "source": [
        "DataLoader:   \n",
        "[1]https://stackoverflow.com/questions/73191999/when-to-use-prepare-data-vs-setup-in-pytorch-lightning\n",
        "\n",
        "[2] https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/213\n",
        "[3] https://scikit-image.org/docs/dev/auto_examples/transform/plot_ssim.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BG3fo48ZmyVb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "MoUQ94FuqlBZ",
        "YGvkwaBQudfO",
        "Mlc-199bb6nw",
        "QZzPAw6d3D2X"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cf8aa1c0581e4b959801d7d1bdd095c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_33572b969b1b42dab2f8ef5f96d8b194",
              "IPY_MODEL_7708d45207fb4eb787edfd38aaf9af93",
              "IPY_MODEL_f0c0b02314664d48a4c5a8d30d1b1766"
            ],
            "layout": "IPY_MODEL_9189285bebb64081808b8b72fd6c266c"
          }
        },
        "33572b969b1b42dab2f8ef5f96d8b194": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca289ee35e444cd0a4d0f95c52384b9a",
            "placeholder": "​",
            "style": "IPY_MODEL_981fe890794546009b530e73b05da56a",
            "value": "Sanity Checking DataLoader 0: 100%"
          }
        },
        "7708d45207fb4eb787edfd38aaf9af93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48bf7a1b4a4c4e5abb70bef341ed8e3a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ce3674b96b534b43b4d1fedf248ea1d7",
            "value": 2
          }
        },
        "f0c0b02314664d48a4c5a8d30d1b1766": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d318988ceff949c58df86dfe6043503c",
            "placeholder": "​",
            "style": "IPY_MODEL_8b20e7a95ab54d48a08f83dad86d2784",
            "value": " 2/2 [00:00&lt;00:00,  3.20it/s]"
          }
        },
        "9189285bebb64081808b8b72fd6c266c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "ca289ee35e444cd0a4d0f95c52384b9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "981fe890794546009b530e73b05da56a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48bf7a1b4a4c4e5abb70bef341ed8e3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce3674b96b534b43b4d1fedf248ea1d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d318988ceff949c58df86dfe6043503c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b20e7a95ab54d48a08f83dad86d2784": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c930d9037b94fc7b6fb263f46bb5253": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c6cacf0c9d7149f1a74a7494f241051e",
              "IPY_MODEL_33dd9a051082468294e5aa259a0f2ed6",
              "IPY_MODEL_a59ca0583de5443489e2f43e8de374fc"
            ],
            "layout": "IPY_MODEL_76a6773957904eb4881a03a595543a6a"
          }
        },
        "c6cacf0c9d7149f1a74a7494f241051e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6851889c74264d098b10bc8bb777898c",
            "placeholder": "​",
            "style": "IPY_MODEL_24c46a5e5fe842c787d252bbc1f3c2e2",
            "value": "Epoch 0:  18%"
          }
        },
        "33dd9a051082468294e5aa259a0f2ed6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f8e1ede676b43d89563d42724741300",
            "max": 3301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a189de1e24014aa6995e77541b77398e",
            "value": 580
          }
        },
        "a59ca0583de5443489e2f43e8de374fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3283c91a5c0042bf82edf1ac4a35f85e",
            "placeholder": "​",
            "style": "IPY_MODEL_efe0040d2ccd46e2bf66427c372cd081",
            "value": " 580/3301 [22:29&lt;1:45:32,  2.33s/it, v_num=4, D_loss_T1=0.0591, D_loss_T2=0.0948, G_loss=1.58e+3]"
          }
        },
        "76a6773957904eb4881a03a595543a6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "6851889c74264d098b10bc8bb777898c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24c46a5e5fe842c787d252bbc1f3c2e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f8e1ede676b43d89563d42724741300": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a189de1e24014aa6995e77541b77398e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3283c91a5c0042bf82edf1ac4a35f85e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efe0040d2ccd46e2bf66427c372cd081": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}